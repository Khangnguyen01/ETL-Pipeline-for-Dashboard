# âš¡ Quick Start Guide

**Get your pipeline running in 3 minutes!**

---

## ğŸ“¦ What You Need

1. âœ… Docker Desktop installed ([Download here](https://www.docker.com/products/docker-desktop))
2. âœ… Google Cloud service account JSON key

---

## ğŸš€ 3-Step Setup

### Step 1: Clone & Navigate
```bash
git clone https://github.com/Khangnguyen01/ETL-Pipeline-for-Dashboard.git
cd ETL-Pipeline-for-Dashboard
```

### Step 2: Add Your Secrets

Create **ONE file**: `application_default_credentials.json`

```
ğŸ“ ETL-Pipeline-for-Dashboard/
   â”œâ”€â”€ dags/
   â”œâ”€â”€ models/
   â”œâ”€â”€ docker-compose.yml
   â””â”€â”€ ğŸ”’ application_default_credentials.json  â† PUT YOUR GCP KEY HERE
```

**How to get this file:**
1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Navigate: **IAM & Admin** â†’ **Service Accounts**
3. Create/Select account with these roles:
   - `BigQuery Data Editor`
   - `BigQuery Job User` 
   - `BigQuery Read Session User`
4. Click **Keys** â†’ **Add Key** â†’ **Create New Key** â†’ **JSON**
5. Download and rename it to `application_default_credentials.json`
6. Place in project root

### Step 3: Launch!
```bash
docker-compose up -d
```

**That's it!** ğŸ‰

Wait 2-3 minutes for initialization, then:
- **Airflow UI**: http://localhost:8080
- **Login**: `airflow` / `airflow`

---

## ğŸ¯ First Pipeline Run

1. Open http://localhost:8080
2. Login: `airflow` / `airflow`
3. Find DAG: `dbt_pipeline`
4. Click **â–¶ Trigger DAG**
5. Watch it run! âœ¨

---

## âš™ï¸ Configuration (Optional)

### Update Your GCP Project ID

Edit `profiles.yml` (line 7):
```yaml
project: wool-away  # â† Change to YOUR-PROJECT-ID
```

### Custom Environment Variables

Create `.env` file (optional):
```bash
AIRFLOW_UID=50000
GCP_PROJECT_ID=your-project-id
```

---

## ğŸ“Š What Gets Created

After successful run, check BigQuery:
- **Dataset**: `dev`
  - **Schema**: `staging` (70+ tables)
  - **Schema**: `mart` (7+ tables)

---

## ğŸ” Common Commands

```bash
# Start services
docker-compose up -d

# Stop services
docker-compose down

# View logs
docker-compose logs -f airflow-scheduler

# Check status
docker-compose ps

# Enter container
docker exec -it <container-name> bash

# Inside container - run dbt manually
cd /opt/airflow/dbt
dbt run --select staging.*
```

---

## ğŸ› Quick Troubleshooting

### âŒ Error: "Permission denied"
```bash
chmod 600 application_default_credentials.json
```

### âŒ Error: "Project not found"
Update `profiles.yml` with correct GCP project ID

### âŒ Error: "Docker won't start"
- Open Docker Desktop
- Increase memory to 8GB+ (Settings â†’ Resources)

### âŒ Error: "DAG not showing"
Wait 2-3 minutes for initialization, then refresh browser

---

## ğŸ“š Need More Details?

- **Full Documentation**: See [README.md](./README.md)
- **Configuration Guide**: See [CONFIGURATION_GUIDE.md](./CONFIGURATION_GUIDE.md)
- **Troubleshooting**: See README.md â†’ Troubleshooting section

---

## âœ… Success Checklist

- [ ] Docker Desktop running
- [ ] `application_default_credentials.json` in project root
- [ ] `profiles.yml` has correct project ID
- [ ] `docker-compose up -d` succeeded
- [ ] http://localhost:8080 accessible
- [ ] Logged in as `airflow`/`airflow`
- [ ] `dbt_pipeline` DAG visible
- [ ] First DAG run successful âœ¨

---

**You're all set! Happy data transforming! ğŸ‰**

