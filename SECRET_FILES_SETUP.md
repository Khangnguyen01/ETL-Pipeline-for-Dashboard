# ğŸ”’ Secret Files Setup Guide

This guide shows you exactly which files you need to add before running Docker.

---

## ğŸ“ Required Files Checklist

After cloning the repository, you need to add these files:

```
ETL-Pipeline-for-Dashboard/
â”‚
â”œâ”€â”€ ğŸ“‚ dags/                             âœ… Already in repo
â”œâ”€â”€ ğŸ“‚ models/                           âœ… Already in repo
â”œâ”€â”€ ğŸ“‚ macros/                           âœ… Already in repo
â”œâ”€â”€ ğŸ“„ docker-compose.yml                âœ… Already in repo
â”œâ”€â”€ ğŸ“„ profiles.yml                      âš ï¸  UPDATE THIS
â”‚
â”œâ”€â”€ ğŸ”’ application_default_credentials.json   âŒ YOU MUST ADD
â””â”€â”€ ğŸ”’ .env                                    ğŸŸ¡ OPTIONAL
```

---

## 1ï¸âƒ£ Required: GCP Service Account Key

### File: `application_default_credentials.json`

**ğŸ“ Location:** Project root (same folder as `docker-compose.yml`)

**ğŸ¯ Purpose:** Authenticates your pipeline with Google BigQuery

**ğŸ“¥ How to Get This File:**

#### Step 1: Go to Google Cloud Console
ğŸ‘‰ https://console.cloud.google.com

#### Step 2: Navigate to Service Accounts
```
â˜° Navigation Menu 
  â†’ IAM & Admin 
    â†’ Service Accounts
```

#### Step 3: Create or Select Service Account
**Option A - Create New:**
```
1. Click "CREATE SERVICE ACCOUNT"
2. Name: dbt-pipeline-service
3. Description: Service account for dbt ETL pipeline
4. Click "CREATE AND CONTINUE"
```

**Option B - Use Existing:**
```
Select an existing service account that has BigQuery access
```

#### Step 4: Grant Required Roles
Select these roles:
- âœ… `BigQuery Data Editor`
- âœ… `BigQuery Job User`
- âœ… `BigQuery Read Session User`

Click "CONTINUE" â†’ "DONE"

#### Step 5: Download JSON Key
```
1. Click on the service account name
2. Go to "KEYS" tab
3. Click "ADD KEY" â†’ "Create new key"
4. Select "JSON"
5. Click "CREATE"
```

A JSON file will download automatically.

#### Step 6: Rename and Place File
```bash
# Rename the downloaded file to:
application_default_credentials.json

# Move it to project root:
ETL-Pipeline-for-Dashboard/
â””â”€â”€ ğŸ”’ application_default_credentials.json  â† HERE
```

**âœ… File Should Look Like:**
```json
{
  "type": "service_account",
  "project_id": "your-project-id",
  "private_key_id": "...",
  "private_key": "-----BEGIN PRIVATE KEY-----\n...",
  "client_email": "dbt-pipeline@your-project.iam.gserviceaccount.com",
  "client_id": "...",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  ...
}
```

---

## 2ï¸âƒ£ Required: Update profiles.yml

### File: `profiles.yml`

**ğŸ“ Location:** Project root

**ğŸ¯ Purpose:** Tells dbt which BigQuery project to use

**âœï¸ What to Change:**

Open `profiles.yml` and update line 7:

```yaml
dbt_dev:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: oauth
      project: wool-away  # â† CHANGE THIS to your GCP project ID
      dataset: dev
      location: US
      threads: 8
      timeout_seconds: 6000
      keyfile: /home/airflow/.config/gcloud/application_default_credentials.json
```

**ğŸ“ Find Your Project ID:**
1. Go to [GCP Console](https://console.cloud.google.com)
2. Look at the top bar - your project name is displayed
3. Click the dropdown â†’ copy the "Project ID" (not "Project Name")

Example:
```
Project Name: My Analytics Project
Project ID: my-analytics-123456  â† Use this one
```

---

## 3ï¸âƒ£ Optional: Environment Variables

### File: `.env`

**ğŸ“ Location:** Project root

**ğŸ¯ Purpose:** Custom configuration (most defaults work fine)

**ğŸ“‹ When You Need This:**
- You want to change Airflow port (default: 8080)
- You want to customize database passwords
- You need specific environment variables

**ğŸ“ How to Create:**

```bash
# Copy the example file
cp .env.example .env

# Edit with your values
nano .env
```

**Example `.env`:**
```bash
# Required
AIRFLOW_UID=50000
GCP_PROJECT_ID=my-analytics-123456

# Optional (defaults work fine)
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
```

**Most users don't need this file!** The defaults work great.

---

## âœ… Final File Structure

After setup, you should have:

```
ETL-Pipeline-for-Dashboard/
â”‚
â”œâ”€â”€ ğŸ”’ application_default_credentials.json  âœ… Added by you
â”œâ”€â”€ ğŸ“„ profiles.yml                          âœ… Updated by you
â”œâ”€â”€ ğŸ”’ .env                                  ğŸŸ¡ Optional
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml                    âœ… From repo
â”œâ”€â”€ ğŸ“„ Dockerfile                            âœ… From repo
â”œâ”€â”€ ğŸ“„ dbt_project.yml                       âœ… From repo
â”œâ”€â”€ ğŸ“„ requirements.txt                      âœ… From repo
â”œâ”€â”€ ğŸ“„ packages.yml                          âœ… From repo
â”‚
â”œâ”€â”€ ğŸ“‚ dags/                                 âœ… From repo
â”œâ”€â”€ ğŸ“‚ models/                               âœ… From repo
â”œâ”€â”€ ğŸ“‚ macros/                               âœ… From repo
â”œâ”€â”€ ğŸ“‚ tests/                                âœ… From repo
â””â”€â”€ ...
```

---

## ğŸš€ Ready to Launch!

Once you have:
- âœ… `application_default_credentials.json` in project root
- âœ… `profiles.yml` updated with your project ID

**Run:**
```bash
docker-compose up -d
```

**Then access:**
- Airflow UI: http://localhost:8080
- Login: `airflow` / `airflow`

---

## ğŸ”’ Security Reminders

### âœ… DO:
- âœ… Keep `application_default_credentials.json` secure
- âœ… Set file permissions: `chmod 600 application_default_credentials.json`
- âœ… Add it to `.gitignore` (already done)
- âœ… Never commit credentials to Git

### âŒ DON'T:
- âŒ Share your credentials file
- âŒ Commit it to Git (it's gitignored)
- âŒ Email it or post it online
- âŒ Store it in public locations

---

## ğŸ†˜ Troubleshooting

### âŒ "File not found: application_default_credentials.json"
**Solution:** Make sure file is in project root, same folder as `docker-compose.yml`
```bash
ls -la application_default_credentials.json
```

### âŒ "Permission denied"
**Solution:** Fix file permissions
```bash
chmod 600 application_default_credentials.json
```

### âŒ "Invalid JSON"
**Solution:** Re-download the key from GCP Console - file may be corrupted

### âŒ "Project not found"
**Solution:** Update `project:` in `profiles.yml` with correct GCP project ID

### âŒ "Access denied to BigQuery"
**Solution:** Service account needs these roles:
- BigQuery Data Editor
- BigQuery Job User
- BigQuery Read Session User

---

## ğŸ“š Next Steps

1. âœ… Added secret files
2. âœ… Updated profiles.yml
3. ğŸ‘‰ See [QUICKSTART.md](./QUICKSTART.md) to launch Docker
4. ğŸ‘‰ See [README.md](./README.md) for full documentation

---

**Questions?** Check [CONFIGURATION_GUIDE.md](./CONFIGURATION_GUIDE.md) for more details.

